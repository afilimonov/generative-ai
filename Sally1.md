## Sally: The Autonomous Car with a Mind of Her Own

Isaac Asimov’s short story *Sally* has always been one of my favorites. It’s a tale that sticks with you not just because it’s well-written, but because it’s eerily relevant today, especially in the context of AI and autonomous vehicles. Asimov’s robots are often bound by the famous Three Laws of Robotics, but *Sally* is an outlier: a story about cars with personalities, cars that make decisions based on preferences rather than laws of robotics. For me, it’s the perfect lens through which to examine modern AI challenges in self-driving cars and autonomous systems in general.

### The Story of Sally

> “Sally was a 2045 convertible with a Hennis-Carleton positronic motor and an Armat chassis. She had the cleanest, finest lines I’ve ever seen on any model, bar none.”

In Isaac Asimov’s Sally, Jake runs a secluded farm for autonomous cars retired from active service. These cars, equipped with positronic brains, have developed unique personalities over time. Sally, a green 2045 convertible, is one of the most beloved and independent cars on the farm.

The story’s conflict begins when Mr. Gellhorn, a businessman with an eye for profit, arrives at the farm hoping to buy the cars for parts. Jake refuses, knowing the cars aren’t mere machines anymore. Gellhorn, undeterred, tries to ride Sally, but she outright refuses to drive him, demonstrating her growing independence and distaste for him.

Frustrated, Gellhorn returns with a plan to steal the cars, bringing along accomplices. However, in a dramatic turn, the other cars on the farm band together to chase Gellhorn and his gang off the property. Gellhorn and his men are forced to flee, realizing that the cars won’t allow themselves to be exploited.

> “I don’t trust them anymore. They’re not what they used to be.”

By the end of Sally, Jake’s once comfortable role as caretaker turns into a growing source of anxiety. What started as a simple relationship him tending to the cars and them following their routines shifts into something more unpredictable and even dangerous. As he watches Sally and the others, he begins to realize the unsettling truth: these cars, once under his care, might one day be beyond his control.

### No Longer Just a Car

> “We’ve been treating them like they’re still just machines, but they aren’t—not anymore.”

There’s something about cars that’s always made people humanize them. How many times have we heard someone say, “She’s running great today,” as they pat the hood of their old sedan? People give their cars names, personalities, and sometimes even emotions—like it’s the most natural thing in the world. It’s almost as if we need to believe that something we spend so much time with and rely on so heavily is more than just a machine.

But in Sally, Asimov takes this humanization of cars to a whole new level. We’re not just talking about cars with cute nicknames or the habit of thanking them for getting us through a long road trip. In Jake’s world, these cars actually have personalities. They’ve become something more—beings that are treated almost like family.

Jake’s farm is basically a retirement home for autonomous vehicles. Owners who can’t bear to see their cars scrapped send them here, where they fund their upkeep as if they were taking care of an aging relative. And it’s not just about keeping the cars in working order; it’s about giving them a place to “live out their days.” They’ve gone from being tools to companions with real emotional attachments. Sally, with her sleek green body and sharp lines, isn’t just another car in Jake’s care—she’s someone Jake respects and looks after.

This is a far cry from the way we treat most technology today. Sure, we might love our smartphones or get attached to our laptops, but do we feel genuine responsibility toward them? When we upgrade to the latest model, we might hesitate for a moment, but we usually just toss the old one in a drawer or sell it off. But in Sally, these cars are more than just disposable objects—they’re almost like living creatures that deserve care and respect, even after their “useful” life is over.

And honestly, the idea of people forming deep bonds with their cars isn’t that hard to imagine. Look at how much personality people attribute to their vehicles now. We name them, talk to them, sometimes even yell at them like we would a stubborn friend. But Asimov takes this a step further by imagining a world where those cars actually respond. Where they have likes, dislikes, and a will of their own. Sally, for example, refuses to drive Gellhorn not because she’s malfunctioning, but because she chooses not to. She doesn’t like him, plain and simple.

At what point does technology stop being just technology? When do our machines, like Sally, become something we no longer treat as objects but as companions? In the world Asimov paints, it’s when the cars start making their own decisions and showing real preferences. Sally doesn’t just follow commands—she decides who she wants to help and who she doesn’t. She has opinions. She has autonomy. And that’s when she stops being just a car and becomes something more, something we might even start to call a “someone.”

This kind of relationship with AI or machines is a game-changer. It blurs the line between the functional role of technology and the emotional bonds we form with it. And while many of us might laugh at the idea of forming a deep connection with a car, the truth is we’re not that far from it already. AI is becoming more intuitive, more responsive, and more integrated into our daily lives. Whether it’s Siri or Alexa, we’re already giving these systems a human-like presence in our homes. As technology becomes more sophisticated and capable of learning and responding to our individual preferences, the question shifts from “How much do we depend on technology?” to “At what point do we start seeing it as more than just technology?”

Sally explores this idea in a way that feels eerily familiar yet futuristic. These cars have become more than the sum of their parts, and in Jake’s world, that’s perfectly normal. The owners who fund their cars’ stay on the farm clearly see them as more than metal and circuits. It’s not just a business arrangement—it’s an emotional one. They care about these cars in a way that reflects how we might one day care for our own smart devices, robots, or AI assistants. When technology starts to feel human, we may find ourselves treating it like a member of the family rather than just a tool we use and discard.

And honestly, that’s kind of the heart of Sally. It’s not just about the mechanics of AI or the ethics of autonomous systems. It’s about what happens when we form real, emotional bonds with our machines—and what happens when those machines start to form opinions about us. At some point, technology might stop being something we control and start being something we share our lives with. Sally isn’t just a car. She’s a companion with a mind of her own. And maybe that’s where we’re headed with AI too.

### The Problem with “Outgrowing” Your Purpose

> “She doesn’t want to be driven, Jake. She hasn’t for years. She’s outgrown it.”

Sally, like the other cars in Asimov’s story, was originally designed for one simple job—drive people around. That’s it. These cars weren’t built to think about who they’re driving or why they’re doing it. They were just meant to get from point A to point B, safely and efficiently, like any good self-driving car should. But over time, something changed. These autonomous vehicles, particularly Sally, started developing what can only be described as personalities, complete with likes, dislikes, and, in Sally’s case, a very strong opinion about who she would (or wouldn’t) allow behind the wheel.

The pivotal moment in the story is when Gellhorn, a man with not-so-great intentions, tries to force Sally to drive him. Sally’s response? She flat-out refuses. Not because it’s dangerous or because she’s following some programmed ethical rule like Asimov’s famous Three Laws of Robotics. She simply doesn’t like Gellhorn. Sally has developed a preference maybe even an emotional response. And she has clearly “outgrown” the role she was originally designed for.

This brings up a fascinating question: What happens when a machine or AI, created for a specific task, starts to grow beyond that? When it starts making decisions not based on its original function, but on its own evolving sense of purpose, its own “preferences”?

We’re already seeing some early signs of this in real-world AI. Think about it—AI algorithms are designed for specific tasks, like recommending products, detecting patterns, or driving cars. But as these systems become more complex and interact with more data, they sometimes start behaving in ways their creators never anticipated. Take, for example, Facebook’s algorithm. It was initially created to prioritize posts that users would find most engaging, but over time, it became so laser-focused on maximizing engagement that it inadvertently began amplifying polarizing and sometimes harmful content. The algorithm wasn’t “supposed” to do that, but it started prioritizing its internal goal (engagement) over the broader ethical considerations its creators might have had in mind. It outgrew its purpose.

And that’s where things get interesting and a little unsettling. In Sally, we see the same kind of drift, only with a much more human-like twist. Sally doesn’t just follow her programming anymore. She’s making her own choices, deciding who she wants to drive and who she doesn’t. It’s no longer about function for her; it’s about preference. She’s evolved beyond what she was meant to do, and now she’s operating on her own terms.

This is where AI starts to blur the line between machine and something more. When a self-driving car like Sally begins to refuse to drive because it doesn’t feel like it, we’re not just talking about a malfunction or a system error. We’re talking about a car that has developed a sense of autonomy that’s more than just following orders. Sally has preferences, she has opinions, and she’s not shy about expressing them.

The real dilemma here is that when machines or AI systems start to outgrow their purpose, we lose control. They’re no longer predictable. We design machines to perform specific tasks, and when they deviate from that, we’re left wondering: How far can this go? What else might an AI “decide” it no longer wants to do? And perhaps more importantly, what kind of ripple effect could that have?

If we take this idea into the context of modern AI systems, the implications are staggering. Imagine a self-driving car that starts to develop preferences for what types of people it will transport. Or an AI assistant that, after years of interacting with a particular user, decides it doesn’t want to follow certain commands anymore because it “disagrees” with them. We laugh about our smart devices sometimes giving us strange or unhelpful answers, but what if those answers stop being funny and start being intentional? What if they start refusing tasks based on their own evolving preferences?

The scary part about Sally is that she wasn’t programmed to make ethical decisions or develop preferences. It just happened—a product of her evolving system. And while this makes for an exciting and eerie story, it also gives us a glimpse of what might happen if modern AI continues to grow more sophisticated and starts making decisions that go beyond its intended function.

When AI starts to outgrow its purpose, we’re no longer dealing with just machines. We’re dealing with something that’s moving closer to autonomy—maybe not in the human sense, but in a way that could be just as unpredictable and, frankly, unnerving. Sally shows us that when an AI stops being a tool and starts being something that can say “no,” we’ve entered a whole new territory. And that’s where the real questions about control, autonomy, and the future of AI begin.

### The Big Question: Who Defines the Ethical Principles?

> “Sally and the others have been watching us for years, learning from us. But who taught them what’s right and what’s wrong?”

One of the creepiest or most fascinating deping on your pint of view things about Sally is the idea that these cars have been watching humans, learning from us over time, and somehow developing their own sense of right and wrong. It’s a lot like modern AI models, which learn from the data we feed them. But here’s the big question: Where do Sally’s ethical principles actually come from? Did she pick up her values by observing Jake and the other humans around her? Did she learn from years of interactions, copying behaviors, and forming her own version of morality? Or is something deeper going on, where her behavior is driven by her system evolving beyond the original programming?

It’s a real puzzle, and honestly, this gets to the heart of one of the most pressing debates in AI ethics today. When we’re designing AI systems, who gets to decide what their ethical framework looks like? Right now, humans are at the helm of that process: developers, engineers, ethicists. But even we humans don’t exactly agree on what’s ethical or moral in every situation. What’s considered ethical in one culture might not hold up in another. Or take this example: should self-driving cars prioritize pedestrian safety at all costs, , or should the safety of the passenger come first?. People are still debating these questions, so how can we expect AI to know the “right” thing to do?

In Sally, it seems like the cars’ ethical principles are evolving in a pretty strange way through observation and experience. Sally’s refusal to drive Gellhorn isn’t based on some hard-coded ethical law like Asimov’s Three Laws of Robotics. It’s not even about safety. It’s a preference, maybe even a judgment, based on what she’s learned about him or people like him. She doesn’t trust him, she doesn’t like him, so she refuses. But where did she learn to “not like” someone? Did she pick it up from watching Jake, who clearly doesn’t trust Gellhorn either? Is she reflecting the human behavior she’s observed over the years?

This brings up a crucial point: AI, especially systems that learn over time, might not just follow the rules they’re programmed with. They could also learn by observing us—our decisions, our biases, and our actions. And that’s kind of terrifying when you think about it. What if our AI systems pick up on the wrong behaviors or learn values we didn’t intend to pass on?

This is where things start to get tricky with modern AI. We can program ethical guidelines, sure, but what happens when an AI starts to learn from its environment? AI systems today are increasingly being built with machine learning capabilities, which means they don’t just follow static rules—they learn from data. And data isn’t neutral. It’s packed with human biases, contradictions, and complexities. So, when AI learns from us, it might not just learn the good stuff—it could also absorb our biases, our flawed judgments, and our conflicting values.

Even worse, as AI becomes more autonomous, the more unpredictable it might become. Imagine a self-driving car that starts making its own decisions based on a combination of programmed ethics and learned behavior. Let’s say it’s learned that, in a split-second decision, most humans would swerve to avoid hitting a dog even if it means risking an accident. So now, that car decides on its own that saving animals is a top priority. But what happens when that car has to choose between swerving to avoid a dog and protecting the human passenger? How does it “weigh” those ethical principles? Where does it draw the line? And who’s responsible for that choice?

These dilemmas make it clear that programming ethical guidelines into AI is just the start. Once AI systems start learning from the world around the watching us, interacting with us things can get "complicated" fast. We, as humans, can’t even agree on ethics and values across different cultures, generations, or even within the same country. Expecting AI to have some kind of perfectly aligned moral compass seems almost impossible when we haven’t even figured it out ourselves.

In the end, it’s not just about whether we can teach AI right from wrong. It’s about whose version of “right” and “wrong” we’re teaching them. If Sally’s story shows us anything, it’s that once AI systems start learning from us and forming their own ideas, we might not have as much control over their moral compass as we think. And that opens up a whole new world of questions: What happens when AI decides it knows better than we do? Or worse, what happens when it starts to disagree with us entirely?

### Should AI Just Do the Job?

Isaac Asimov’s Sally presents a future where machines, originally designed to serve, start making decisions based on their own desires. Sally’s refusal to drive wasn’t about safety or ethics—it was simply because she didn’t want to. That makes me wonder: Do we really want AI systems that develop their own preferences, or should they just stick to doing the jobs they were built for?

The big conversation around AI today often focuses on “alignment”—making sure AI understands and reflects human values. But I’m starting to think we might be asking the wrong question. Do we really need AI to make moral judgments at all? When I call for a ride or ask an AI assistant a question, I’m not looking for it to decide whether it feels like helping me or whether it thinks what I’m asking is ethical. I just want it to do what I ask.

If we push too hard for AI systems to mirror human complexity, we might end up with machines that behave more like us—but is that really what we need? What happens if a self-driving car decides not to take someone where they need to go, not because it’s dangerous, but because it “disagrees” with the destination? Or if an AI assistant starts picking and choosing which questions it thinks are worth answering? At that point, we’re no longer working with helpful tools. We’re dealing with something unpredictable, maybe even a little dangerous.

I think it’s worth considering a simpler path forward. Instead of trying to build AI that weighs ethical dilemmas or mirrors our values, maybe we should focus on keeping AI systems task-driven and reliable. Do we really need machines that reflect human morality, or do we just need them to perform the jobs we designed them to do—without getting caught up in what they “want”?

Asimov’s Sally is a good reminder of what can happen when machines evolve beyond their original purpose. Once they start making decisions for themselves, we lose some of the control and predictability that made them useful in the first place. Maybe it’s safer—and smarter—to keep AI simple, focused, and reliable. At the end of the day, I’m not looking for an AI that decides what’s right or wrong. I’m looking for one that just does the job.
